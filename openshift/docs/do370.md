# do370

## 0 - General command

	oc login -u admin -p redhat https://api.ocp4.example.com:6443

## 1 - Setup Openshift Data Foundation (ODF)

### 1.1 - Label

	oc get node/worker01
	oc desc node worker01 => node-role.kubernetes.io/worker=
	oc get nodes -l node-role.kubernetes.io/worker=
	oc label nodes -l node-role.kubernetes.io/worker= cluster.ocs.openshift.io/openshift-storage=
	
	oc whoami --show-console
	oc debug node/worker01 -- lsblk --paths --nodeps

### 1.2.A - Method 1 

Operators > OperatorHub: Local Storage
Create Local Volume Discovery 

Operators > OperatorHub: OpenShift Container Storage
Create StorageCluster
		Internal - Attached Devices
		lso-volumeset
		Stretch Cluster cleared

### 1.2.B - Method 2



#### a. Installing the Local Storage operator Group

Create project
	oc adm new-project openshift-local-storage
	oc project openshift-local-storage

Local Storage Operator Group


	apiVersion: operators.coreos.com/v1
	kind: OperatorGroup
	metadata:
		name: openshift-local-storage
		namespace: openshift-local-storage
	spec:
		targetNamespaces:
		- openshift-local-storage

Subscription

	apiVersion: operators.coreos.com/v1alpha1
	kind: Subscription
	metadata:
		name: local-storage-operator
		namespace: openshift-local-storage
	spec:
		channel: "4.7"
		name: local-storage-operator
		source: redhat-operators
		sourceNamespace: openshift-marketplace

Discovery LocalVolume

	apiVersion: local.storage.openshift.io/v1alpha1
	kind: LocalVolumeDiscovery
	metadata:
		name: auto-discover-devices
		namespace: openshift-local-storage
	spec
		nodeSelector:
			nodeSelectorTerms:
			- matchExpressions:
				- key: kubernetes.io/hostname
					operator: In
					values
					- worker01
					- worker02
					- worker03

Discovery LocalVolumeSet
	
	apiVersion: local.storage.openshift.io/v1alpha1
	kind: LocalVolumeSet
	metadata:
		name: lso-volumeset
		namespace: openshift-local-storage
	spec:
		deviceInclusionSpec:
			deviceTypes:
			- disk
			- part
			minSize: 0Ti
		nodeSelector:
			nodeSelectorTerms:
			- matchExpressions:
				- key: kubernetes.io/hostname
					operator: In
					values:
					- worker01
					- worker02
					- worker03
				storageClassName: lso-volumeset
				volumeMode: Block
				maxDeviceCount: 1

#### b. Installing the Openshift Container Storage operator 

OperatorGroup

	apiVersion: operators.coreos.com/v1
	kind: OperatorGroup
	metadata:
		name: openshift-storage
		namespace: openshift-storage
	spec:
		targetNamespaces:
		- openshift-storage

	oc apply -f lso-operatorgroup.yml

Subscription

	apiVersion: operators.coreos.com/v1alpha1
	kind: Subscription
	metadata:
		name: ocs-operator
		namespace: openshift-storage
	spec:
		channel: "stable-4.7"
		name: ocs-operator
		source: redhat-operators
		sourceNamespace: openshift-marketplace

	oc apply -f lso-subscription.yml

StorageCluster

	apiVersion: ocs.openshift.io/v1
	kind: StorageCluster
	metadata:
		name: ocs-storagecluster
		namespace: openshift-storage
	spec:
		monDataDirHostPath: /var/lib/rook
		storageDeviceSets:
		- count: 1
			dataPVCTemplate:
			spec:
				accessModes:
				- ReadWriteOnce
				resources:
					requests:
						storage: "1"
				storageClassName: lso-volumeset
				volumeMode: Block
		name: ocs-deviceset-lso-volumeset
		replica: 3
	version: 4.7.0

	oc apply -f lso-storagecluster.yml

### 1.3 - Validation

	oc adm new-project openshift-local-storage
	oc adm new-project openshift-storage

	oc get localvolumediscovery -n openshift-local-storage
	oc get localvolumediscoveryresults -n openshift-local-storage
	oc get localvolumeset -n openshift-local-storage

	oc get operatorgroups -n openshift-local-storage
	oc get subscriptions -n openshift-local-storage
	oc get clusterserviceversions -n openshift-local-storage


	oc get operatorgroups -n openshift-storage
	oc get subscriptions -n openshift-storage
	oc get clusterserviceversions -n openshift-storage

	oc get storagecluster -n openshift-storage
	oc get storageclasses
	
## 2 - Configuring OpenShift Cluster Services 

### 2.1 - Configuring the Internal Image Registry

	oc get storageclasses -o name
	
	oc apply -f obc-registry.yml
	---
	apiVersion: objectbucket.io/v1alpha1
	kind: ObjectBucketClaim
	metadata:
		name: noobaa-registry
		namespace: openshift-image-registry
	spec:
		additionalConfig:
			bucketclass: noobaa-default-bucket-class
		generateBucketName: noobaa-registry
		storageClassName: openshift-storage.noobaa.io

	oc get -n openshift-image-registry objectbucketclaim/noobaa-registry
	oc get secrets -l app=noobaa -n openshift-image-registry
	oc extract secret/noobaa-registry -n openshift-image-registry
	oc create secret generic image-registry-private-configuration-user -n openshift-image-registry \
	--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY="$(cat AWS_ACCESS_KEY_ID)"  \
	--from-literal=REGISTRY_STORAGE_S3_SECRETKEY="$(cat AWS_SECRET_ACCESS_KEY)" 

	oc get -n openshift-image-registry objectbucketclaim/noobaa-registry -o jsonpath='{.spec.bucketName}{"\n"}'
	noobaa-registry-038ca5ee-d9ed-4b20-997a-c72058af2426

	oc get route/s3 -n openshift-storage -o jsonpath='{.spec.host}{"\n"}'
	s3-openshift-storage.apps.ocp4.example.com

	imageregistry-patch.yaml
	---
	apiVersion: imageregistry.operator.openshift.io/v1
	kind: Config
	metadata:
		name: cluster
	spec:
		storage:
			managementState: Managed
			pvc: null
			s3:
				bucket: noobaa-registry-038ca5ee-d9ed-4b20-997a-c72058af2426
				region: us-east-1
				regionEndpoint: https://s3-openshift-storage.apps.ocp4.example.com

	oc patch configs.imageregistry/cluster --type=merge --patch-file=imageregistry-patch.yaml
	oc get configs.imageregistry/cluster -o jsonpath='{.spec.storage}' | jq .
	oc get pods -n openshift-image-registry -l docker-registry=default

	oc new-project services-registry
	oc new-app --name hello https://github.com/RedHatTraining/DO280-apps --context-dir hello-world-nginx
	oc logs -f buildconfig/hello

	oc apply -f job-awscli.yaml

### 2.2 - Configuring Monitoring

	oc exec -n openshift-monitoring statefulset/prometheus-k8s -c prometheus -- df -h /prometheus
	oc exec -n openshift-monitoring statefulset/alertmanager-main -c alertmanager -- df -h /alertmanager
	metrics-storage.yml
	prometheusK8s:
		retention: 7d
		volumeClaimTemplate:
			spec:
				storageClassName: ocs-storagecluster-ceph-rbd
				resources:
					requests:
						storage: 40Gi
	alertmanagerMain:
		volumeClaimTemplate:
			spec:
				storageClassName: ocs-storagecluster-ceph-rbd
				resources:
					requests:
						storage: 20Gi

	oc create -n openshift-monitoring configmap cluster-monitoring-config --from-file config.yaml=metrics-storage.yml
	oc get statefulsets -n openshift-monitoring

## 3 - Configuring Application Workloads

### 3.1 - Identifying Ceph Components

	oc get pods -n openshift-storage
	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'
	oc exec -it pod/rook-ceph-operator-548bcdc79f-xcgjb -c rook-ceph-operator -n openshift-storage  -- /bin/bash
	ceph -c /var/lib/rook/openshift-storage/openshift-storage.config health
	ceph -c /var/lib/rook/openshift-storage/openshift-storage.config -s
	ceph -c /var/lib/rook/openshift-storage/openshift-storage.config df

	oc adm must-gather --image=registry.redhat.io/ocs4/ocs-must-gather-rhel8:v4.7 --dest-dir=must-gather

### 3.2 - Configuring Applications 

File pvc.yaml

	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:
		name: image-tool
	spec:
		accessModes:
		- ReadWriteMany
		storageClassName: ocs-storagecluster-cephfs
		resources:
			requests:
				storage: 1Gi

File deployment.yaml

		volumeMounts:
		- name: image-tool-storage
			mountPath: "/var/storage"
	volumes:
	- name: image-tool-storage
		persistentVolumeClaim:
			claimName: image-tool

Create app image-tool

	oc apply -f pvc.yaml -f deployment.yaml
	oc exec -it deployment/image-tool-pvc -- df -h /var/storage
	oc logs deployment/image-tool-pvc 
	
Create app nginx and verify 2 app have same pvc

	oc get pods -l app=image-tool
	oc get pods -l app=nginx