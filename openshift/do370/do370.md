# do370

## 0 - General command

	oc login -u admin -p redhat https://api.ocp4.example.com:6443

## 1 - Setup Openshift Data Foundation (ODF)

### 1.1 - Label

	oc get node/worker01
	oc desc node worker01 => node-role.kubernetes.io/worker=
	oc get nodes -l node-role.kubernetes.io/worker=
	oc label nodes -l node-role.kubernetes.io/worker= cluster.ocs.openshift.io/openshift-storage=
	
	oc whoami --show-console
	oc debug node/worker01 -- lsblk --paths --nodeps

### 1.2.A - Method 1 

Operators > OperatorHub: Local Storage
Create Local Volume Discovery 

Operators > OperatorHub: OpenShift Container Storage
Create StorageCluster
		Internal - Attached Devices
		lso-volumeset
		Stretch Cluster cleared

### 1.2.B - Method 2



#### a. Installing the Local Storage operator Group

Create project
	oc adm new-project openshift-local-storage
	oc project openshift-local-storage

Local Storage Operator Group


	apiVersion: operators.coreos.com/v1
	kind: OperatorGroup
	metadata:
		name: openshift-local-storage
		namespace: openshift-local-storage
	spec:
		targetNamespaces:
		- openshift-local-storage

Subscription

	apiVersion: operators.coreos.com/v1alpha1
	kind: Subscription
	metadata:
		name: local-storage-operator
		namespace: openshift-local-storage
	spec:
		channel: "4.7"
		name: local-storage-operator
		source: redhat-operators
		sourceNamespace: openshift-marketplace

Discovery LocalVolume

	apiVersion: local.storage.openshift.io/v1alpha1
	kind: LocalVolumeDiscovery
	metadata:
		name: auto-discover-devices
		namespace: openshift-local-storage
	spec
		nodeSelector:
			nodeSelectorTerms:
			- matchExpressions:
				- key: kubernetes.io/hostname
					operator: In
					values
					- worker01
					- worker02
					- worker03

Discovery LocalVolumeSet
	
	apiVersion: local.storage.openshift.io/v1alpha1
	kind: LocalVolumeSet
	metadata:
		name: lso-volumeset
		namespace: openshift-local-storage
	spec:
		deviceInclusionSpec:
			deviceTypes:
			- disk
			- part
			minSize: 0Ti
		nodeSelector:
			nodeSelectorTerms:
			- matchExpressions:
				- key: kubernetes.io/hostname
					operator: In
					values:
					- worker01
					- worker02
					- worker03
				storageClassName: lso-volumeset
				volumeMode: Block
				maxDeviceCount: 1

#### b. Installing the Openshift Container Storage operator 

OperatorGroup

	apiVersion: operators.coreos.com/v1
	kind: OperatorGroup
	metadata:
		name: openshift-storage
		namespace: openshift-storage
	spec:
		targetNamespaces:
		- openshift-storage

	oc apply -f lso-operatorgroup.yml

Subscription

	apiVersion: operators.coreos.com/v1alpha1
	kind: Subscription
	metadata:
		name: ocs-operator
		namespace: openshift-storage
	spec:
		channel: "stable-4.7"
		name: ocs-operator
		source: redhat-operators
		sourceNamespace: openshift-marketplace

	oc apply -f lso-subscription.yml

StorageCluster

	apiVersion: ocs.openshift.io/v1
	kind: StorageCluster
	metadata:
		name: ocs-storagecluster
		namespace: openshift-storage
	spec:
		monDataDirHostPath: /var/lib/rook
		storageDeviceSets:
		- count: 1
			dataPVCTemplate:
			spec:
				accessModes:
				- ReadWriteOnce
				resources:
					requests:
						storage: "1"
				storageClassName: lso-volumeset
				volumeMode: Block
		name: ocs-deviceset-lso-volumeset
		replica: 3
	version: 4.7.0

	oc apply -f lso-storagecluster.yml

### 1.3 - Validation

	oc adm new-project openshift-local-storage
	oc adm new-project openshift-storage

	oc get localvolumediscovery -n openshift-local-storage
	oc get localvolumediscoveryresults -n openshift-local-storage
	oc get localvolumeset -n openshift-local-storage

	oc get operatorgroups -n openshift-local-storage
	oc get subscriptions -n openshift-local-storage
	oc get clusterserviceversions -n openshift-local-storage


	oc get operatorgroups -n openshift-storage
	oc get subscriptions -n openshift-storage
	oc get clusterserviceversions -n openshift-storage

	oc get storagecluster -n openshift-storage
	oc get storageclasses
	
## 2 - Configuring OpenShift Cluster Services 

### 2.1 - Configuring the Internal Image Registry

	oc get storageclasses -o name

Create a claim (a secret is created also)

	oc apply -f obc-registry.yml
	---
	apiVersion: objectbucket.io/v1alpha1
	kind: ObjectBucketClaim
	metadata:
		name: noobaa-registry
		namespace: openshift-image-registry
	spec:
		additionalConfig:
			bucketclass: noobaa-default-bucket-class
		generateBucketName: noobaa-registry
		storageClassName: openshift-storage.noobaa.io

	oc get -n openshift-image-registry objectbucketclaim/noobaa-registry
	oc get secrets -l app=noobaa -n openshift-image-registry
	oc extract secret/noobaa-registry -n openshift-image-registry

Create a user 

	oc create secret generic image-registry-private-configuration-user -n openshift-image-registry \
	--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY="$(cat AWS_ACCESS_KEY_ID)"  \
	--from-literal=REGISTRY_STORAGE_S3_SECRETKEY="$(cat AWS_SECRET_ACCESS_KEY)" 

	oc get -n openshift-image-registry objectbucketclaim/noobaa-registry -o jsonpath='{.spec.bucketName}{"\n"}'
	noobaa-registry-038ca5ee-d9ed-4b20-997a-c72058af2426

External endpoint route for NooBaa MCG object storage

	oc get route/s3 -n openshift-storage -o jsonpath='{.spec.host}{"\n"}'
	s3-openshift-storage.apps.ocp4.example.com

The null value in the pvc key indicates that this section will be deleted when the patch is applied. The S3 library uses the region parameter to create the appropriate AWS URL for
the S3 bucket. However, this value is overridden when specifying a custom URL in the regionEndpoint parameter

	imageregistry-patch.yaml
	---
	apiVersion: imageregistry.operator.openshift.io/v1
	kind: Config
	metadata:
		name: cluster
	spec:
		storage:
			managementState: Managed
			pvc: null
			s3:
				bucket: noobaa-registry-038ca5ee-d9ed-4b20-997a-c72058af2426
				region: us-east-1
				regionEndpoint: https://s3-openshift-storage.apps.ocp4.example.com



	oc patch configs.imageregistry/cluster --type=merge --patch-file=imageregistry-patch.yaml
	oc get configs.imageregistry/cluster -o jsonpath='{.spec.storage}' | jq .
	oc get pods -n openshift-image-registry -l docker-registry=default

	oc new-project services-registry
	oc new-app --name hello https://github.com/RedHatTraining/DO280-apps --context-dir hello-world-nginx
	oc logs -f buildconfig/hello

	oc apply -f job-awscli.yaml
	oc patch deployment/hello --type merge --patch '{"spec":{"template":{"spec":{"nodeSelector":{"env":"qa"}}}}}'
	oc label nodes -l env=qa env- 

### 2.2 - Configuring Monitoring

	oc exec -n openshift-monitoring statefulset/prometheus-k8s -c prometheus -- df -h /prometheus
	oc exec -n openshift-monitoring statefulset/alertmanager-main -c alertmanager -- df -h /alertmanager

Create file metrics-storage.yml

	metrics-storage.yml
	prometheusK8s:
		retention: 7d
		volumeClaimTemplate:
			spec:
				storageClassName: ocs-storagecluster-ceph-rbd
				resources:
					requests:
						storage: 40Gi
	alertmanagerMain:
		volumeClaimTemplate:
			spec:
				storageClassName: ocs-storagecluster-ceph-rbd
				resources:
					requests:
						storage: 20Gi

Create configmap name config.yaml

	oc create -n openshift-monitoring configmap cluster-monitoring-config --from-file config.yaml=metrics-storage.yml
	oc get statefulsets -n openshift-monitoring

	oc exec -n openshift-monitoring statefulset/prometheus-k8s -c prometheus -- df -h /prometheus
	oc exec -n openshift-monitoring statefulset/alertmanager-main -c alertmanager -- df -h /alertmanager

## 3 - Configuring Application Workloads

	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'

### 3.1 - Identifying Ceph Components

Show logs pod

	oc get pods -n openshift-storage
	oc logs rook-ceph-osd-0-6cbf78cd77-t7dnr -c osd -n openshift-storage
	oc logs rook-ceph-mon-0-6cbf78cd77-t7dnr -c mon -n openshift-storage
	
Obtain the cluster health

	oc exec -it pod/rook-ceph-operator-548bcdc79f-xcgjb -c rook-ceph-operator -n openshift-storage  -- /bin/bash
	bash-4.4$ ceph -c /var/lib/rook/openshift-storage/openshift-storage.config health
	bash-4.4$ ceph -c /var/lib/rook/openshift-storage/openshift-storage.config -s
	bash-4.4$ ceph -c /var/lib/rook/openshift-storage/openshift-storage.config df

Obtain the cluster logs bundle

	oc adm must-gather --image=registry.redhat.io/ocs4/ocs-must-gather-rhel8:v4.7 --dest-dir=must-gather

### 3.2 - Configuring Applications to use File Storage

Show route

	oc get route/image-tool-pvc -o jsonpath='{.spec.host}'

File pvc.yaml

	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:
		name: image-tool
	spec:
		accessModes:
		- ReadWriteMany
		storageClassName: ocs-storagecluster-cephfs
		resources:
			requests:
				storage: 1Gi

File deployment.yaml
	
	apiversion: apps/v1
	kind: Deployment
	metadata:
		labels:
			app: image-tool-pvc
		name: image-tool-pvc
  spec:
		template:
			metadata:
				labels:
					app: mage-tool-pvc
			spec:
				containers: 
					volumeMounts:
					- name: image-tool-storage
						mountPath: "/var/storage"
				volumes:
				- name: image-tool-storage
					persistentVolumeClaim:
						claimName: image-tool

Create app image-tool

	oc apply -f pvc.yaml -f deployment.yaml
	oc exec -it deployment/image-tool-pvc -- df -h /var/storage
	oc logs deployment/image-tool-pvc 
	
Create app nginx and verify 2 app have same pvc

	oc get pods -l app=image-tool
	oc get pods -l app=nginx

### 3.3 - Configuring Applications to use Block Storage

Project workloads-block
	
	oc new-project workloads-block

File pvc.yaml

	---
	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:
		name: famous-quotes-db
	spec:
		accessModes:
		- ReadWriteOnce
		storageClassName: ocs-storagecluster-ceph-rbd
		resources:
			requests:
				storage: 1Gi

	oc apply -f ~/DO370/labs/workloads-block/pvc.yaml
	oc process mariadb-persistent -n openshift \
-p DATABASE_SERVICE_NAME=famous-quotes-db \
-p MYSQL_USER=myuser -p MYSQL_PASSWORD=r3dh4t \
-p MYSQL_DATABASE=quotes \
| oc create -f -

## 4 - Managing 

	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'

### 4.1 - Quota

Create Developer user and add rule create pvc

	oc adm policy add-role-to-user basic-user developer -n capacity-quotas
	oc adm policy who-can create persistentvolumeclaims
	admin
	system:admin
	oc adm policy add-cluster-role-to-user cluster-admin developer
	oc adm policy who-can create persistentvolumeclaims
	admin
	developer <<<<<
	system:admin


	pvc-quota.yaml 
	apiVersion: v1
	kind: ResourceQuota
	metadata:
		name: pvc-quota
	spec:
		hard:
			persistentvolumeclaims: "1"

	oc get ResourceQuotas
	oc login -u developer -p developer https://api.ocp4.example.com:6443
  oc apply -f pvc-test1.yaml
	oc get ResourceQuotas
	oc apply -f pvc-test1.yaml


### 4.2 - Expand pvc

	oc patch pvc/pg-capacity-extend-ge -p '{"spec":{"resources":{"requests": {"storage": "150M"}}}}'
	oc edit pvc/pg-capacity-extend-ge
	oc rollout latest dc/pg-capacity-extend-ge

### 4.3 - Expand hard drive

	oc get pods -n openshift-storage -l app=rook-ceph-osd
	oc get localvolumediscoveryresults -n openshift-local-storage
	oc describe localvolumediscoveryresult/discovery-result-worker01 -n openshift-local-storage


# 6 - ODF Object Storage 

	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'

### 6.1 - S3

Create Developer user and add rule create pvc

	oc exec -it deployment/s3-cli -n object-define -- /bin/bash
	aws s3 ls s3://
	object-bucket-8e722bfb-f687-4fa3-820a-bd1ffff08084
	printenv BUCKET_NAME
	object-bucket-8e722bfb-f687-4fa3-820a-bd1ffff08084

	aws s3 ls s3://${BUCKET_NAME} --summarize
	aws s3 cp sample-file.txt s3://${BUCKET_NAME}/
	aws s3 cp s3://${BUCKET_NAME}/sample-file.txt /tmp/
	aws s3 cp sample-file.txt s3://${BUCKET_NAME}/sample-file.txt
	aws s3 cp s3://${BUCKET_NAME}/sample-file.txt s3://${BUCKET_NAME}/second-file.txt
	aws s3 cp s3://${BUCKET_NAME}/second-file.txt /tmp/
	aws s3 ls s3://${BUCKET_NAME} --summarize

Create prefix

	aws s3api put-object --bucket ${BUCKET_NAME} --key prefix
	aws s3 mv s3://${BUCKET_NAME}/prefix s3://${BUCKET_NAME}/my-s3-prefix
	aws s3 ls s3://${BUCKET_NAME}
	aws s3 mv s3://${BUCKET_NAME}/second-file.txt s3://${BUCKET_NAME}/other-file.txt
	aws s3 mv s3://${BUCKET_NAME}/sample-file.txt s3://${BUCKET_NAME}/my-s3-prefix/
	aws s3 ls s3://${BUCKET_NAME} --recursive --summarize

Sync file in folder

	aws s3api put-object --bucket ${BUCKET_NAME} --key icons
	aws s3 sync /usr/share/httpd/icons/ s3://${BUCKET_NAME}/icons/
	aws s3 ls s3://${BUCKET_NAME} --recursive --human-readable --summarize

Delete

	aws s3 rm s3://${BUCKET_NAME}/other-file.txt
	aws s3 rm s3://${BUCKET_NAME}/icons/small/ --recursive

### 6.2 - Object Storage Claim

	oc get route/s3 -n openshift-storage -o jsonpath='{.spec.host}{"\n"}' 
	s3-openshift-storage.apps.ocp4.example.com