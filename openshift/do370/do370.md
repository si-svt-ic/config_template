# do370

## 0 - General command

	oc login -u admin -p redhat https://api.ocp4.example.com:6443
	oc get templates -n openshift

Check parameter list of template

	oc process --parameters mariadb-persistent -n openshift 

## 1 - Setup Openshift Data Foundation (ODF)

### 1.1 - Label

	oc get node/worker01
	oc desc node worker01 => node-role.kubernetes.io/worker=
	oc get nodes -l node-role.kubernetes.io/worker=
	oc label nodes -l node-role.kubernetes.io/worker= cluster.ocs.openshift.io/openshift-storage=
	
	oc whoami --show-console
	oc debug node/worker01 -- lsblk --paths --nodeps

### 1.2 - Method 1 

Operators > OperatorHub: Local Storage
Create Local Volume Discovery 

Operators > OperatorHub: OpenShift Container Storage
Create StorageCluster
		Internal - Attached Devices
		lso-volumeset
		Stretch Cluster cleared

### 1.2 - Method 2


Install openshift local storage Operator and apply needed yaml files

	oc adm new-project openshift-local-storage
	oc project openshift-local-storage

- Local Storage Operator Group
- Subscription
- LocalVolumeDiscovery to search all logical volume(drives)
- LocalVolumeSet to create a Set of volumes 

Installing the Openshift Container Storage operator and apply needed yaml files

	oc adm new-project openshift-storage
	oc project openshift-storage

- OperatorGroup
- Subscription
- StorageCluster

### 1.3 - Validation

	oc get localvolumediscovery -n openshift-local-storage
	oc get localvolumediscoveryresults -n openshift-local-storage
	oc get localvolumeset -n openshift-local-storage

	oc get operatorgroups -n openshift-local-storage
	oc get subscriptions -n openshift-local-storage
	oc get clusterserviceversions -n openshift-local-storage


	oc get operatorgroups -n openshift-storage
	oc get subscriptions -n openshift-storage
	oc get clusterserviceversions -n openshift-storage

	oc get storagecluster -n openshift-storage
	oc get storageclasses
	
## 2 - Configuring OpenShift Cluster Services 

### 2.1 - Configuring the Internal Image Registry

Create a object storage claim (a secret is created also): noobaa-registry

	oc get secrets -l app=noobaa -n openshift-image-registry
	oc extract secret/noobaa-registry -n openshift-image-registry

Create a secret image-registry-private-configuration-user

	oc create secret generic image-registry-private-configuration-user -n openshift-image-registry \
	--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY="$(cat AWS_ACCESS_KEY_ID)"  \
	--from-literal=REGISTRY_STORAGE_S3_SECRETKEY="$(cat AWS_SECRET_ACCESS_KEY)" 

Path the image registry cluster config 

	oc patch configs.imageregistry/cluster --type=merge --patch-file=imageregistry-patch.yaml
	oc get configs.imageregistry/cluster -o jsonpath='{.spec.storage}' | jq .
	oc get pods -n openshift-image-registry -l docker-registry=default

Add data to new bucket

	oc new-project services-registry
	oc new-app --name hello https://github.com/RedHatTraining/DO280-apps --context-dir hello-world-nginx
	oc logs -f buildconfig/hello

	oc apply -f job-awscli.yaml
	oc patch deployment/hello --type merge --patch '{"spec":{"template":{"spec":{"nodeSelector":{"env":"qa"}}}}}'
	oc label nodes -l env=qa env- 

### 2.2 - Configuring Monitoring

	oc exec -n openshift-monitoring statefulset/prometheus-k8s -c prometheus -- df -h /prometheus
	oc exec -n openshift-monitoring statefulset/alertmanager-main -c alertmanager -- df -h /alertmanager

Create file metrics-storage.yml [File at](config-do370/solutions/services-metrics/metrics-storage.yml)
Create configmap name config.yaml

	oc create -n openshift-monitoring configmap cluster-monitoring-config --from-file config.yaml=metrics-storage.yml
	oc get statefulsets -n openshift-monitoring

	oc exec -n openshift-monitoring statefulset/prometheus-k8s -c prometheus -- df -h /prometheus
	oc exec -n openshift-monitoring statefulset/alertmanager-main -c alertmanager -- df -h /alertmanager

## 3 - Configuring Application Workloads

	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'

### 3.1 - Identifying Ceph Components

Show logs pod

	oc get pods -n openshift-storage
	oc logs rook-ceph-osd-0-6cbf78cd77-t7dnr -c osd -n openshift-storage
	oc logs rook-ceph-mon-0-6cbf78cd77-t7dnr -c mon -n openshift-storage
	oc logs csi-rbdplugin-86dpc              -c csi-rbdplugin -n openshift-storage

Obtain the cluster health

	oc exec -it pod/rook-ceph-operator-548bcdc79f-xcgjb -c rook-ceph-operator -n openshift-storage  -- /bin/bash
	$ ceph -c /var/lib/rook/openshift-storage/openshift-storage.config health
	$ ceph -c /var/lib/rook/openshift-storage/openshift-storage.config -s
	$ ceph -c /var/lib/rook/openshift-storage/openshift-storage.config df

Obtain the cluster logs bundle

	oc adm must-gather --image=registry.redhat.io/ocs4/ocs-must-gather-rhel8:v4.7 --dest-dir=must-gather

### 3.2 - Configuring Applications to use File Storage

Show route

	oc get route/image-tool-pvc -o jsonpath='{.spec.host}'

Create app image-tool

	oc apply -f pvc.yaml -f deployment.yaml
	oc exec -it deployment/image-tool-pvc -- df -h /var/storage
	oc logs deployment/image-tool-pvc 
	
Create app nginx and verify 2 app have same pvc

	oc get pods -l app=image-tool
	oc get pods -l app=nginx

### 3.3 - Configuring Applications to use Block Storage

Project workloads-block
	
	oc new-project workloads-block
	oc apply -f ~/DO370/labs/workloads-block/pvc.yaml
	oc process mariadb-persistent -n openshift -p DATABASE_SERVICE_NAME=famous-quotes-db -p MYSQL_USER=myuser -p MYSQL_PASSWORD=r3dh4t -p MYSQL_DATABASE=quotes | oc create -f -

### 3.4 - Storage Class

	oc new-app --name=pg-workload-classes --template=postgresql-persistent-sc -p STORAGECLASS_NAME=ocs-storagecluster-ceph-rbd-xfs -p VOLUME_CAPACITY=150Mi -p POSTGRESQL_USER=student -p POSTGRESQL_PASSWORD=redhat -p POSTGRESQL_DATABASE=workloads-classes -p DATABASE_SERVICE_NAME=pg-workload-classes-ge
	
	oc new-app --name=dev-pg-workload-classes --template=postgresql-persistent-sc -p STORAGECLASS_NAME=pg-development-storage -p VOLUME_CAPACITY=150Mi -p POSTGRESQL_USER=developer -p POSTGRESQL_PASSWORD=devel -p POSTGRESQL_DATABASE=dev-workloads-classes -p DATABASE_SERVICE_NAME=dev-pg-workload-classes-ge



## 4 - Managing 

	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'

### 4.1 - Quota

Create Developer user and add rule create pvc

	oc adm policy add-role-to-user basic-user developer -n capacity-quotas
	oc adm policy who-can create persistentvolumeclaims
	admin
	system:admin

	oc adm policy add-cluster-role-to-user cluster-admin developer
	oc adm policy who-can create persistentvolumeclaims
	admin
	developer <<<<<
	system:admin

	oc get ResourceQuotas
	oc login -u developer -p developer https://api.ocp4.example.com:6443
  oc apply -f pvc-test1.yaml
	oc apply -f pvc-test2.yaml


### 4.2 - Expand pvc
	
	oc create -f DO370/labs/capacity-extend/postgresql-persistent-ge.json
	oc new-app --name=pg-capacity-extend --template=postgresql-persistent-sc -p STORAGECLASS_NAME=ocs-storagecluster-ceph-rbd -p VOLUME_CAPACITY=30Mi -p POSTGRESQL_USER=student -p POSTGRESQL_PASSWORD=redhat -p POSTGRESQL_DATABASE=capacity-extend-ge -p DATABASE_SERVICE_NAME=pg-capacity-extend-ge

	oc patch pvc/pg-capacity-extend-ge -p '{"spec":{"resources":{"requests": {"storage": "150M"}}}}'
	oc edit pvc/pg-capacity-extend-ge
	oc rollout latest dc/pg-capacity-extend-ge

### 4.3 - Expand hard drive

	oc get pods -l app=rook-ceph-osd -n openshift-storage 
	oc get pv -l kubernetes.io/hostname=worker01 -n openshift-local-storage 

	oc get localvolumediscoveryresults -n openshift-local-storage
	oc describe localvolumediscoveryresult/discovery-result-worker01 -n openshift-local-storage


## 5 - Backup Restore

The configuration map contains the following parameters:

### 5.1 - Backup & Restore

	oc exec -it deployment/mariadb -- /bin/bash
	$ mysql -u ${MYSQL_USER} -p"${MYSQL_PASSWORD}" ${MYSQL_DATABASE}

Create Developer user and add rule create pvc

### 5.2 - Volume Snapshot & Clone

Create Developer user and add rule create pvc

## 6 - ODF Object Storage 

The configuration map contains the following parameters:
• BUCKET_HOST
• BUCKET_PORT
• BUCKET_NAME
• BUCKET_REGION
• BUCKET_SUBREGION
The secret contains the keys required to access the bucket.
• AWS_ACCESS_KEY_ID
• AWS_SECRET_ACCESS_KEY
Object Route
	External Route for incoming request: 
		NooBaa Object: route/s3
		Openshift-Storage: route/ocs-storageclustercephobjectstore

	oc get pod my-pod-name -n my-namespace -o jsonpath='{.spec.containers[*].name}'

### 6.1 - S3

Create Developer user and add rule create pvc

	oc exec -it deployment/s3-cli -n object-define -- /bin/bash
	aws s3 ls s3://
	object-bucket-8e722bfb-f687-4fa3-820a-bd1ffff08084
	printenv BUCKET_NAME
	object-bucket-8e722bfb-f687-4fa3-820a-bd1ffff08084

	aws s3 ls s3://${BUCKET_NAME} --summarize
	aws s3 cp sample-file.txt s3://${BUCKET_NAME}/
	aws s3 cp s3://${BUCKET_NAME}/sample-file.txt /tmp/
	aws s3 cp sample-file.txt s3://${BUCKET_NAME}/sample-file.txt
	aws s3 cp s3://${BUCKET_NAME}/sample-file.txt s3://${BUCKET_NAME}/second-file.txt
	aws s3 cp s3://${BUCKET_NAME}/second-file.txt /tmp/
	aws s3 ls s3://${BUCKET_NAME} --summarize

Create prefix

	aws s3api put-object --bucket ${BUCKET_NAME} --key prefix
	aws s3 mv s3://${BUCKET_NAME}/prefix s3://${BUCKET_NAME}/my-s3-prefix
	aws s3 ls s3://${BUCKET_NAME}
	aws s3 mv s3://${BUCKET_NAME}/second-file.txt s3://${BUCKET_NAME}/other-file.txt
	aws s3 mv s3://${BUCKET_NAME}/sample-file.txt s3://${BUCKET_NAME}/my-s3-prefix/
	aws s3 ls s3://${BUCKET_NAME} --recursive --summarize

Sync file in folder

	aws s3api put-object --bucket ${BUCKET_NAME} --key icons
	aws s3 sync /usr/share/httpd/icons/ s3://${BUCKET_NAME}/icons/
	aws s3 ls s3://${BUCKET_NAME} --recursive --human-readable --summarize

Delete

	aws s3 rm s3://${BUCKET_NAME}/other-file.txt
	aws s3 rm s3://${BUCKET_NAME}/icons/small/ --recursive

### 6.2 - Client access Object Storage Claim

Create:
• Object Bucket Claim Name: my-object-bucket-claim
• Storage Class: openshift-storage.noobaa.io
• Bucket Class noobaa-default-bucket-class

	oc get route/s3 -n openshift-storage -o jsonpath='{.spec.host}{"\n"}' >>>> s3-openshift-storage.apps.ocp4.example.com 
	oc extract configmap/my-object-bucket-claim --to=-
	
	export BUCKET_HOST="s3-openshift-storage.apps.ocp4.example.com"
	export BUCKET_NAME="my-object-bucket-claim-27f4ce2f-13ca-44cc-b8a5-1a747989cfb3"
	export AWS_CA_BUNDLE=/etc/pki/tls/certs/ca-bundle.crt

	aws configure
	cat ~/.aws/credentials
	aws s3 ls s3:// --endpoint-url "https://${BUCKET_HOST}"

Config default S3 endpoint

	pip install awscli-plugin-endpoint
	aws configure set plugins.endpoint awscli_plugin_endpoint
	aws configure --profile default set s3.endpoint_url https://${BUCKET_HOST}
	cat ~/.aws/config
		[default]
		region = us-east-1
		s3 =
		endpoint_url = https://s3-openshift-storage.apps.ocp4.example.com
		[plugins]
		endpoint = awscli_plugin_endpoint

	aws s3 ls s3://
	aws s3 ls s3://${BUCKET_NAME} --summarize

### 6.3 - Application access Object Storage Claim

References:

	https://flask.palletsprojects.com/en/1.1.x/patterns/fileuploads/#uploading-files
	https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html
	https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/

	oc apply -f serviceaccount.yaml -f deployment.yaml -f service.yaml -f route.yaml
	Edit the deployment.yaml
	oc apply -f obc.yaml -f deployment-obc.yaml

### 6.4 - Monitoring

Monitoring > Metrics
	• rate(NooBaa_providers_bandwidth_read_size[2m])
	• rate(NooBaa_providers_bandwidth_write_size[2m])

# Solution

## 1 setup
  ok
## 2 image registry
## 3 resource management 

Request:
  
	In Project project-1, user-A can oc get all, but cannot create resource, edit and delete existing resource	 
	In Project project-1, user-B can create SA
	In Project project-1, user-B cannot elevate the priviliges of the user-A ( ko có quyền cấp quyền)

	In Project project-1, minimum of an invidiual PVC is 1Gi maximum is 3Gi 

Solution:

	oc login -u khanh.chu -p admin123 api.ocp5.svtech.gay:6443
	oc login -u user-B -p redhat123 api.ocp5.svtech.gay:6443

	oc new-project project-1
	oc adm policy add-role-to-user view user-A -n project-1
	
	oc create role sa-create --verb=create --resource=serviceaccount -n project-1
	oc adm policy add-role-to-user sa-create user-B -n project-1

	oc create serviceaccount gitlab-sb

Validate User-B right to impersonate to user-A 

	oc get all -n project-1 --as user-A

Impersonate rule 

=== a.yaml ===
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: impersonate-user-a
rules:
- apiGroups: [""]
  resources: ["users"]
  verbs: ["impersonate"]

=== b.yaml ===
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: impersonate-user-a-binding
subjects:
- kind: User
  name: user-B
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: impersonate-user-a
  apiGroup: rbac.authorization.k8s.io

=== limitrange.yaml ===
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
	namespace: 1
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 2Gi
    min:
      storage: 1Gi


## 4 prometheus
  ok
## 5 storageclass

## 6 quota

Request:

In Project 2, create up to 2 PVC, up to 5Gi
In Project 2, for storageclass a, create up to 1 PVC, up to 2Gi 

Solution:

apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-consumption
	namespace: 2
spec:
  hard:
    persistentvolumeclaims: "2" 
    requests.storage: "5Gi" 
    silver.storageclass.storage.k8s.io/persistentvolumeclaims: "1" 
	silver.storageclass.storage.k8s.io/requests.storage: "1Gi" 
    
    
## 7 troubleshoot
  ok
   - write a deployment with volumemounts, pvc: Mode:Filesystem/rbd 
   - dashy-pvc should bigger than 500Gb

## 8 filestorage
  ok 
   - write a deployment with volumemounts
   - pod log shows : port 5000 => target port: 5000 service, route
## 9 snapshot


	
	oc create secret generic secret1 --from-file my_abc=/path/to/abc.txt 

## 10 clone
